{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reservoir sampling\n",
    "\n",
    "When a stream of data arrives, and we can't store it all, one simple idea is to take a uniform random sample of it.\n",
    "\n",
    "How can we do this efficiently? When we don't initially know the length of the stream, $m$, but we do know $k$, the number of items we want to sample?\n",
    "\n",
    "This is sampling without replacement. To achieve sampling with replacement, we make copies of the reservoir sampler.\n",
    "\n",
    "## Reservoir algorithm\n",
    "\n",
    "```\n",
    "Let S[1..k] be an empty array\n",
    "Let m be 0\n",
    "For each item x\n",
    "    increment m   //this is the stream length\n",
    "    if m <= k\n",
    "        Put x in S[m]\n",
    "    else\n",
    "        Let r be chosen uniformly in [1..m]\n",
    "        If r <= k, S[r] becomes x\n",
    "Output S\n",
    "```\n",
    "\n",
    "Claim: For every item in the stream, the chance that it's actually the sampled item is $\\frac{1}{m}$.\n",
    "\n",
    "# Counters\n",
    "\n",
    "In general, estimating the number of occurrences of something is worth doing efficiently. We might be tracking many many somethings.\n",
    "\n",
    "An obvious starting point is that we can record a count of size $n$ in $\\lceil\\log_2n\\rceil$ bits.\n",
    "\n",
    "But we can in fact approximate this counter with about $\\log\\log n$ bits.\n",
    "\n",
    "## Morris counter algorithm\n",
    "\n",
    "```\n",
    "Let z be 0\n",
    "When a new item arrives:\n",
    "    Flip a coin with Heads probability 1/2^z\n",
    "    If Heads\n",
    "        Increment z\n",
    "Return 2^z-1 as the estimated count\n",
    "```\n",
    "\n",
    "Let $Z_n$ be the random variable corresponding to the value of $z$ after we have seen $n$ items. We'll show that in fact the expected value of $Y_n=2^{Z_n}$ is $n+1$. This means that we can use $2^{Z_n}-1$ as an estimate of $n$, the number of items seen.\n",
    "\n",
    "Proof:\n",
    "\n",
    "Considering the rule for updating $z$, and therefore the relationship between $Z_{n-1}$ and $Z_n$:\n",
    "\n",
    "$$Pr[Z_n=j]=Pr[Z_{n-1}=j](1-\\frac{1}{2^j})+Pr[Z_{n-1}=j-1]\\frac{1}{2^{j-1}}$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$Pr[Z_n=j]=(Pr[Z_{n-1}=j]-Pr[Z_{n-1}=j]\\frac{1}{2^j})+2Pr[Z_{n-1}=j-1]\\frac{1}{2^j}$$\n",
    "\n",
    "and \n",
    "\n",
    "$$Pr[Z_n=j]=Pr[Z_{n-1}=j]+\\frac{1}{2^j}(2Pr[Z_{n-1}=j-1]-Pr[Z_{n-1}=j])$$\n",
    "\n",
    "Applying this to the expectation of $Y_n$\n",
    "\n",
    "$E[2^{Z_n}]=\\sum_jPr[Z_n=j]2^j=\\sum_jPr[Z_{n-1}=j]2^j+\\sum_j(2Pr[Z_{n-1}=j-1]-Pr[Z_{n-1}=j])$\n",
    "\n",
    "The first item on the right hand side is simply $E[2^{Z_{n-1}}]=E[Y_{n-1}]$\n",
    "\n",
    "The second item is $2(Pr[Z_{n-1}=0]+Pr[Z_{n-1}=1]+Pr[Z_{n-1}=2]+\\cdots+Pr[Z_{n-1}=n-1])-(Pr[Z_{n-1}=1]+Pr[Z_{n-1}=2]+\\cdots+Pr[Z_{n-1}=n-1])=2-1=1$\n",
    "\n",
    "Therefore, $E[Y_n]=E[Y_{n-1}]+1$\n",
    "\n",
    "Since the expected value of $Y_1$ is $2$, therefore, $E[Y_n]=n+1$. The expected value of the returned solution is $n$.\n",
    "\n",
    "End of proof!\n",
    "\n",
    "The expected value of $Z_n$ is $\\log_2{(n+1)}$, so the counter uses about $\\log\\log n$ bits.\n",
    "\n",
    "We can also show that the **variance** of $Y_n$ is $\\frac{n(n-1)}{2}$.\n",
    "\n",
    "We can produce a better estimator by running several independent Morris counters and combining their results. This approach has fairly general application in estimation analysis, especially in streaming.\n",
    "\n",
    "## Averaging the counters\n",
    "\n",
    "We will construct an estimator of $n$ that with probability $1-\\delta$ has accuracy $\\epsilon n$.\n",
    "\n",
    "1. We group together several Morris counters and take their mean value. In fact, we use $\\frac{2}{\\epsilon^2}$ of them\n",
    "2. Then we take $c\\log{\\frac{1}{\\delta}}$ such grouped estimators and return the median of those means\n",
    "3. This median of means will provide an estimate with the desired accuracy with high probability\n",
    "\n",
    "Let's focus on the expected value first.\n",
    "\n",
    "Say we have $h=\\frac{2}{\\epsilon^2}$ Morris counters, each has a value $z$.\n",
    "\n",
    "Since $E[2^{Z_n}]=n+1$, if we take the average of the $h$ values of the form $Y=2^z$, we should have a fairly good estimate of $n$.\n",
    "\n",
    "We then subtract $1$, and call this returned value $X$, which is an estimate of the true number of items seen, n.\n",
    "\n",
    "How much does $X$ deviate from this mean value of $n$?\n",
    "\n",
    "$$Pr[|X-n|\\ge\\epsilon n]\\le\\frac{Var(X)}{\\epsilon^2n^2}$$\n",
    "\n",
    "$Var[\\frac{X_1+X_2+\\cdots+X_h}{h}]=\\frac{1}{h^2}Var[X_1+X_2+\\cdots+X_h]=\\frac{1}{h^2}hVar[X]=\\frac{Var[X]}{h}$\n",
    "\n",
    "Now the variance of $X$ is $\\frac{1}{h}$ times the variance of an individual $Y_n$ value, which is $\\frac{n(n-1)}{2}$.\n",
    "\n",
    "Since $h=\\frac{2}{\\epsilon^2}$, the bound is $\\frac{1}{\\epsilon^2n^2}\\frac{\\epsilon^2n(n-1)}{4}<\\frac{1}{4}$.\n",
    "\n",
    "That is, the probability that our estimate is more than $\\epsilon n$ away from the true value $n$ is bounded less than $\\frac{1}{4}$.\n",
    "\n",
    "## Raising the chance of success\n",
    "\n",
    "We now take several, $L=c\\ln\\frac{1}{\\delta}$, of these X values, and return the median of them.\n",
    "\n",
    "Let's show that this median of means is within $\\epsilon n$ of the true value, $n$, with probability $1-\\delta$.\n",
    "\n",
    "### Chernoff bounds\n",
    "\n",
    "Imagine we have a collection of biased coins, with known biases, and we toss them all. We expect the total number of heads $T$ to be about the sum of the individual heads probabilities of the coins, $\\mu$.\n",
    "\n",
    "It turns out that if there are many coins, and they are independent, that the total is very sharply concentrated near the expected value.\n",
    "\n",
    "$\\begin{cases}Pr[T\\ge(1+\\beta)\\mu]\\le e^{-\\beta^2\\mu/3}\\quad\\quad 0<\\beta<1\\\\\n",
    "Pr[T\\ge(1-\\beta)\\mu]\\le e^{-\\beta^2\\mu/2}\\quad\\quad 0<\\beta<1\n",
    "\\end{cases}$\n",
    "\n",
    "The probability of a single bad estimate is at most $\\frac{1}{4}$\n",
    "\n",
    "If the **median** of the $L$ estimates based on the means, $X_1,X_2,\\cdots,X_L$ is bad, this is equivalent to (at least) **half** of the $X_i$ estimators being bad.\n",
    "\n",
    "Since these estimators are independent, it's like flipping $L$ coins with probability of heads $\\frac{1}{4}$ and finding that the number of heads is at least $L/2$.\n",
    "\n",
    "Substituting $\\beta=1$ into those bounds, we find that the probability of the number of heads being at least $L/2$, when $\\mu\\le L/4$, is at most $e^{-L/12}$.\n",
    "\n",
    "If $L=c\\ln{\\frac{1}{\\delta}}$ and $c=12$, then the probability that the median is bad is at most $e^{-\\ln{1/\\delta}}=\\delta$\n",
    "\n",
    "In summary, with something like $24\\frac{1}{\\epsilon^2}\\ln{\\frac{1}{\\delta}}\\log\\log n$ bits of space for all these counters, we have with high probability, $1-\\delta$, an accurate estimate, within additive error $\\epsilon n$ of the true count, $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
